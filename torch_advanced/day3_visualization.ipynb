{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 了解了可以用torchinfo来可视化模型结构，\n",
    "> CNN的可视化，最近正在用Grad-CAM来可视化展示分类模型的显著图，\n",
    "但是对于分割模型的显著图展示有报错，感觉原因是CNN分类模型的最后输出是x个类别，然后反推。但是我用的分割模型，输出是512x512的图。\n",
    "> 最后学习了TensorBoard的使用。可以plt画好图，然后add_figure的方式添加进去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 可视化网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着DL的发展，网络结构越来越复杂。我们也很难确定每一层的输入结构，输出结构以及参数等信息，这样导致我们很难在短时间内完成debug。\n",
    "因此掌握一个可以用来可视化网络结构的工具十分有必要。类似的功能在另一个DL库Keras中可以调用一个叫做model.summary()的API来很方便地实现，\n",
    "调用后就会显示我们的模型参数，输入大小，输出大小，模型的整体参数等，但是在Pytorch中没有这样一种便利的工具帮助我们可视化我们的模型结构。\n",
    "\n",
    "为了解决这个问题，人们开发了torchinfo工具包（torchinfo时由torchsummary和torchsummaryX重构出的库，torchsummary和torchsummaryX已经很久没更新了）。\n",
    "本节将介绍如何使用torchinfo来可视化网络结构。\n",
    "\n",
    "* 可视化网络结构的方法\n",
    "\n",
    "#### 1.1 使用print函数打印模型基础信息\n",
    "我们将使用ResNet18的结构进行展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model=models.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面两步，就得到resnet18的模型结构。在学习torchinfo之前，先直接print(model)的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以发现单纯的print(model),之恩呢得到基础构件的信息，既不能显示出每一层的shape，也不能显示对应参数量的大小，\n",
    "为了解决这些问题，我们就需要介绍今天的主人公torchinfo\n",
    "\n",
    "#### 1.2 使用torchinfo可视化网络结构\n",
    "\n",
    "* torchinfo的安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装方法一\n",
    "pip install torchinfo \n",
    "# 安装方法二\n",
    "conda install -c conda-forge torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torchinfo的使用\n",
    "\n",
    "torchinfo的使用也是十分简单，我们只需要使用torchinfo.summary()就行了，必须的参数分别时model，input_size[batch_size,channel,h,w],更多参数参考[documentation](https://github.com/TylerYep/torchinfo#documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "resnet18=models.resnet18()\n",
    "summary(model,(1,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torchinfo的结构化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "=========================================================================================\n",
    "ResNet                                   --                        --\n",
    "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
    "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
    "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
    "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
    "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
    "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
    "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
    "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
    "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
    "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
    "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
    "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
    "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
    "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
    "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
    "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
    "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
    "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
    "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
    "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
    "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
    "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
    "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
    "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
    "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
    "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
    "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
    "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
    "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
    "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
    "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
    "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
    "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
    "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
    "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
    "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
    "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
    "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
    "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
    "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
    "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
    "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
    "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
    "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
    "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
    "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
    "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
    "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
    "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
    "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
    "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
    "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
    "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
    "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
    "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
    "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
    "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
    "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
    "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
    "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
    "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
    "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
    "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
    "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
    "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
    "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
    "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
    "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
    "├─Linear: 1-10                           [1, 1000]                 513,000\n",
    "=========================================================================================\n",
    "Total params: 11,689,512\n",
    "Trainable params: 11,689,512\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (G): 1.81\n",
    "=========================================================================================\n",
    "Input size (MB): 0.60\n",
    "Forward/backward pass size (MB): 39.75\n",
    "Params size (MB): 46.76\n",
    "Estimated Total Size (MB): 87.11\n",
    "========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到torchinfo提供了更加详细的信息，包括模块信息（每一层的类型、输出shape和参数量）、模型整体的参数量、模型大小、一次前向或者反向传播需要的内存大小等\n",
    "\n",
    "注意：\n",
    "\n",
    "但你使用的是colab或者jupyter notebook时，想要实现该方法，summary()一定是该单元（即notebook中的cell）的返回值，否则我们就需要使用print(summary(...))来可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 CNN卷积层可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN是DL中非常重要的模型结构，广泛地用于图像处理，极大地提升了模型表现，推动了CV的发展和进步，\n",
    "但CNN是一个“黑盒模型”，人们并不知道CNN是如何获得较好表现的，由此带来了DL的可解释性问题。\n",
    "如果能理解CNN工作的方式，人们不仅能够解释所获得的结果，提升模型的鲁棒性，而且还能有针对性地改进CNN的结构以获得进一步的效果提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解CNN的重要一步是可视化，包括可视化特征是如何提取的、提取到的特征的形式以及模型在输入数据上的关注点等。\n",
    "本节从上述三个方面出发，介绍如何在Pytorch的框架下完成CNN模型的可视化。\n",
    "\n",
    "* 可视化CNN卷积核的方法\n",
    "* 可视化CNN特征图的方法\n",
    "* 可视化CNN显著图（class activation map）的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 CNN卷积核可视化\n",
    "\n",
    "卷积核在CNN中负责提取特征，可视化卷积核能够帮助人们理解CNN各个层在提取什么样的特征，进而理解模型的工作原理。\n",
    "例如在Zeiler和Fergus 2013年的[paper](https://arxiv.org/pdf/1311.2901.pdf)中就研究了CNN各个层的卷积核的不同，\n",
    "他们发现靠近输入的层提取的特征是相对简单的结构，而靠近输出的层提取的特征就和图中的实体形状相近了，如下图所示：\n",
    "\n",
    "![layer2](./figures/figures_visualization/layer2.png)\n",
    "\n",
    "![layer3](./figures/figures_visualization/layer3.png)\n",
    "\n",
    "![layer4](./figures/figures_visualization/layer4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Pytorch中可视化卷积核也非常方便，核心在于特定层的卷积核即特定层的模型权重，可视化卷积核就等价于可视化对应的权杖矩阵。\n",
    "下面给出在Pytorch中可视化卷积核的实现方案，以torchvision自带的VGG11模型为例。\n",
    "\n",
    "首先加载模型，并确定模型的层信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '1': ReLU(inplace=True), '2': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), '3': Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '4': ReLU(inplace=True), '5': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), '6': Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '7': ReLU(inplace=True), '8': Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '9': ReLU(inplace=True), '10': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), '11': Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '12': ReLU(inplace=True), '13': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '14': ReLU(inplace=True), '15': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), '16': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '17': ReLU(inplace=True), '18': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), '19': ReLU(inplace=True), '20': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import vgg11\n",
    "\n",
    "model=vgg11(pretrained=False)\n",
    "print(dict(model.features.named_children()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积核对应的应为卷积层（Conv2d），这里以第“3”层为例，可视化对应的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "conv1=dict(model.features.named_children())['3']\n",
    "kernel_set=conv1.weight.detach()\n",
    "num=len(conv1.weight.detach())\n",
    "print(kernel_set.shape)\n",
    "for i in range(0,num):\n",
    "    i_kernel=kernel_set[i]\n",
    "    plt.figure(figsize=(20,17))\n",
    "    if (len(i_kernel)) > 1:\n",
    "        for idx, filer in enumerate(i_kernel):\n",
    "            plt.subplot(9,9,idx+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(filer[:,:].detach(),cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于第'3'层的特征图由64维变为128维，因此共有128*64个卷积核，其中部分卷积核可视化效果如下图\n",
    "\n",
    "![kernel](./figures/figures_visualization/kernel_vis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 CNN特征图可视化方法\n",
    "\n",
    "与卷积核相对那个，输入的原始图像经过每次卷积层得到的数据为特征图，可视化卷积核是为了看模型提取哪些特征，\n",
    "可视化特征图则是为了看模型提取到的特征是什么样子的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取特征图的方法有很多种，可以从输入开始，逐层做前向传播，直到想要的特征图将其返回。\n",
    "尽管这种方法可行，但是有些麻烦了。\n",
    "在Pytorch中，提供了一个专用的接口使得网络在前向传播过程中能够获得到特征图，这个接口的名称非常形象，叫做'hook'。\n",
    "\n",
    "可以想象这样的场景，数据通过网络向前传播，网络某一层我们预先设置了一个钩子，数据传播过后钩子上会留下数据在这一层的样子，\n",
    "读取钩子的信息就是这一层的特征图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook(object):\n",
    "    def __init__(self):\n",
    "        self.module_name=[]\n",
    "        self.features_in_hook=[]\n",
    "        self.feature_out_hook=[]\n",
    "    \n",
    "    def __call__(self,module,fea_in,fea_out):\n",
    "        print(\"hooker working\",self)\n",
    "        self.module_name.append(module.__class__)\n",
    "        self.features_in_hook.append(fea_in)\n",
    "        self.feature_out_hook.append(fea_out)\n",
    "        return None\n",
    "\n",
    "def plot_feature(model,idx):\n",
    "    hh=Hook()\n",
    "    model.features[idx].register_forward_hook(hh)\n",
    "\n",
    "    forward_model(modle,False)\n",
    "    print(hh.module_name)\n",
    "    print(hh.features_in_hook[0][0].shape)\n",
    "    print(hh.feature_out_hook[0].shape)\n",
    "\n",
    "    out1=hh.feature_out_hook[0]\n",
    "\n",
    "    total_ft=out1.shape[1]\n",
    "    first_item=out1[0].cpu().clone()\n",
    "\n",
    "    plt.figure(figsize=(20,17))\n",
    "\n",
    "    for ftidx in range(total_ft):\n",
    "        if ftidx > 99:\n",
    "            break\n",
    "        ft=first_item[ftidx]\n",
    "        plt.subplot(10,10,ftidx+1)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.imshow(ft[:,:].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们首先实现了一个hook类，之后在plot_feature函数中，将该hook类的对象注册到要进行可视化的网络的某层中。\n",
    "model在进行前向传播的时候会调用hook的__call__函数，我们也就是在那里存储了当前层的输入和输出。\n",
    "这里的features_out_hook是一个list，每次前向传播一次，都是调用一次，也就是features_out_hook长度会增加1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 CNN class activation map可视化方法\n",
    "\n",
    "class ativation map (CAM)的作用是判断哪些变量对模型来说是重要的，在CNN可视化的场景下，即判断图像中哪些像素点对预测结果是重要的。\n",
    "除了确定重要的像素点，人们也会对重要区域的梯度感兴趣，因此在CAM的基础上也进一步改进得到了Grad-CAM（以及诸多变种）。\n",
    "CAM和Grad-CAM的示例如下图所示：\n",
    "\n",
    "![cam](./figures/figures_visualization/cam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比可视化卷积核与可视化特征图，CAM系列可视化更为直观，能够一目了然地确定重要区域，进而进行可解释性分析或模型优化改进。\n",
    "CAM系列操作的实现可以通过开源工具包pytorch-grad-cam来实现。\n",
    "\n",
    "* 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vgg11,resnet18,resnet101,resnext101_32x8d\n",
    "import matplotlib.pyplot as plot_feature\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "model=vgg11(pretrained=True)\n",
    "img_path='./dog.jpg'\n",
    "# resize操作是为了和传入神经网络训练图片大小一致\n",
    "img=Image.open(img_path).resize((224,224))\n",
    "# 需要将原始图片转为np.float32格式并且在0-1之间\n",
    "rgb_img=np.float32(img)/255\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dog](./figures/figures_visualization/dog.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM,ScoreCAM,GradCAMPlusPlus,AblationCAM,XGradCAM,FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "target_layers=[model.features[-1]]\n",
    "# 选取合适的类激活图，但是ScoreCAM和AblationCAM需要batch_size\n",
    "cam=GradCAM(model=model,target_layers=target_layers)\n",
    "targets=[ClassifierOutputTarget(preds)]\n",
    "grayscale_cam=cam(input_tensor=img_tensor,targets=targets)\n",
    "grayscale_cam=grayscale_cam[0,:]\n",
    "cam_img=show_cam_on_image(rgb_img,grayscale_cam,use_rgb=True)\n",
    "print(type(cam_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grad_cam](./figures/figures_visualization/cam_dog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 使用FlashTorch快速实现CNN可视化\n",
    "\n",
    "聪明的你可能要问了，已经202x年了，难道还要我们手把手去写各种CNN可视化的代码吗？答案当然是否定的。随着PyTorch社区的努力，目前已经有不少开源工具能够帮助我们快速实现CNN可视化。这里我们介绍其中的一个——[FlashTorch](https://github.com/MisaOgura/flashtorch)。\n",
    "\n",
    "* 安装\n",
    "\n",
    "pip install flashtorch\n",
    "\n",
    "- 可视化梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download example images\n",
    "# !mkdir -p images\n",
    "# !wget -nv \\\n",
    "#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \\\n",
    "#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg   \\\n",
    "#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg    \\\n",
    "#    -P /content/images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from flashtorch.utils import apply_transforms, load_image\n",
    "from flashtorch.saliency import Backprop\n",
    "\n",
    "model=models.alexnet(pretrained=True)\n",
    "backprop = Backprop(model)\n",
    "\n",
    "image = load_image('/content/images/great_grey_owl.jpg')\n",
    "owl=apply_transforms(image)\n",
    "\n",
    "target_class=24\n",
    "backprop.visualize(owl,target_class,guided=True,use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ft-gradient](./figures/figures_visualization/ft_gradient.png)\n",
    "\n",
    "- 可视化卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from flashtorch.activmax import GradientAscent\n",
    "\n",
    "model=models.vgg16(pretrained=True)\n",
    "g_ascent=GradientAscent(model.features)\n",
    "\n",
    "# specify layer and filter info\n",
    "conv5_1=model.features[24]\n",
    "conv5_1_filters=[45,271,363,489]\n",
    "\n",
    "g_ascent.visualize(conv5_1,conv5_1_filters,title=\"VGG16:conv5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ft-activate](./figures/figures_visualization/ft_activate.png)\n",
    "\n",
    "#### 本节参考内容：\n",
    "\n",
    "- https://andrewhuman.github.io/cnn-hidden-layout_search\n",
    "- https://github.com/jacobgil/pytorch-grad-cam\n",
    "- https://github.com/MisaOgura/flashtorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 使用TensorBoard可视化训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程的可视化在DL模型训练中扮演着重要的角色。学习的过程是一个优化的过程，我们需要找到最优的点作为训练过程的输出产物。\n",
    "一般来说，我们会结合训练集的损失函数和验证集的损失函数，绘制两条损失函数的曲线来确定训练的终点，找到对应的模型用于测试。\n",
    "那么除了记录训练中每个epoch的loss值，能否实时观察损失函数曲线的变化，及时捕捉模型的变化呢？\n",
    "\n",
    "此外，我们也希望可视化其它内容，如输入数据（尤其图片）、模型结构、参数分布等，\n",
    "这些对于我们值debug中查找问题来源非常重要（比如输入数据和我们想象的是否一致）。\n",
    "\n",
    "TensorBoard作为一款可视化工具能偶满足上面提到的各种需求。TensorBoard由TensorFlow团队开发，\n",
    "最早和TensorFlow配合使用，后来广泛应用于各种DL框架的可视化中。\n",
    "\n",
    "本节学习要求：\n",
    "\n",
    "* 安装TensorBoard工具\n",
    "* 了解TensorBoard可视化的基本逻辑\n",
    "* 掌握利用TensorBoard实现训练过程可视化\n",
    "* 掌握利用TensorBoard完成其他内容的可视化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 TensorBoard安装\n",
    "\n",
    "在已安装PyTorch的环境中使用pip安装即可：\n",
    "\n",
    "`pip install tensorboard`\n",
    "\n",
    "也可以使用PyTorch自带的tensorboard工具，此时不需要额外安装tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 TensorBoard可视化的基本逻辑\n",
    "可以将TensorBoard看作一个记录员，它可以记录我们指定的数据，包括模型每一层的feature map，权重，以及训练loss等等\n",
    "TensorBoard将记录下来的内容保存中一个用户指定的文件夹里，程序不断运行中TensorBoard会不断记录。记录下的内容可以通过网页的形式加以可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 TensorBoard的配置与启动\n",
    "\n",
    "在使用TensorBoard前，我们需要先制定一个文件夹供TensorBoard保存记录下来的数据。然后调用tensorboard中的SummaryWriter作为上述“记录员”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer=SummaryWriter('./runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的操作实例化SummaryWritter为变量writer，并制定writer的输出目录为当前目录下的“runs”目录。也就是说，之后tensorboard记录下来的内容都会保存在runs\n",
    "\n",
    "如果使用Pytorch自带的tensorboard，则采用如下方式import："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否可以手动往runs文件夹里添加数据用于可视化，或者把runs文件夹里的数据放到其它机器上可视化呢？\n",
    "答案是可以的。只要数据被记录，可以将这个数据分享给其他人，其他人在安装了tensorboard的情况下，就会看到你分享的数据。\n",
    "\n",
    "启动tensorboard也很简单，在命令行中输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=/path/to/logs/ --port=xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中“path/to/logs/”是指定的保存tensorboard记录结果的文件路径（等价于上面的“./runs“，port是外部访问tensorboard的端口号，\n",
    "可以通过访问ip:port访问tensorboard，这一操作和jupter notebook的使用类似。 如果不是在服务器远程使用的话则不需要配置port。\n",
    "\n",
    "有时，为了tensorboard能够不断地在后台运行，也可以使用nohup命令或者tmux工具来运行tensorboard。\n",
    "\n",
    "下面，我们将模型DL模型训练过程，来介绍如何利用tensorboard来可视化其中的各个部分\n",
    "\n",
    "#### 3.4 TensorBoard模型结构可视化：\n",
    "\n",
    "首先定义模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5)\n",
    "        self.adaptive_pool=nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(64,32)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(32,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.adaptive_pool(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        y=self.sigmoid(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "model=Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化模型的思路和7.1中介绍的方法一样，都是给定一个输入数据，前向传播后得到模型的结构，再通过TensorBoard进行可视化，使用add_graph：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, input_to_model = torch.rand(1, 3, 224, 224))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示结果如下（其中框内部分初始会显示为“Net\"，需要双击后才会展开）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tb_model](./figures/figures_visualization/tb_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 TensorBoard图像可视化\n",
    "\n",
    "当我们做图像相关的任务时，可以方便地将所处理的图片在tensorboard中进行可视化展示\n",
    "\n",
    "* 对于单张图片的显示使用add_image\n",
    "* 对于多张图片的显示使用add_images\n",
    "* 有时需要使用torchvision.utils.make_grid将多张图片拼成一张图片后，用writer.add_image显示\n",
    "\n",
    "这里我们使用torchvision的CIFAR10数据集为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transforms_train=transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "transfroms_test=transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_data=datasets.CIFAR10(\".\",train=True,download=True,transform=transforms_train)\n",
    "test_data=datasets.CIFAR10(\".\",train=False,download=True,transform=transfroms_test)\n",
    "train_loadr=DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader=DataLoader(test_data,batch_size=64)\n",
    "\n",
    "images,labels=next(iter(train_loadr))\n",
    "\n",
    "# 仅查看一张图片\n",
    "writer=SummaryWriter('./pytorch_tb')\n",
    "writer.add_image('images[0]',images[0])\n",
    "writer.close()\n",
    "\n",
    "# 将多张图片拼接成一张图片，中间用黑色网格分割\n",
    "# create grid of images\n",
    "writer=SummaryWriter('./pytorch_tb')\n",
    "img_grid=torchvision.utils.make_grid(images)\n",
    "writer.add_image('image_grid',img_grid)\n",
    "writer.close()\n",
    "\n",
    "# 将多张图片直接输入\n",
    "writer=SummaryWriter('./pytorch_tb')\n",
    "writer.add_images(\"images\",images,global_step=0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依次运行上面三组可视化（注意不要同时在notebook的一个单元格内运行），得到的可视化结果如下（最后运行的结果在最上面）\n",
    "\n",
    "\n",
    "![tb_images](./figures/figures_visualization/tb_images.png)\n",
    "\n",
    "![tb_image_gird](./figures/figures_visualization/tb_image_grid.png)\n",
    "\n",
    "![tb_image](./figures/figures_visualization/tb_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外注意上方的menu部分，刚刚只有“GRAPHS”栏对应模型的可视化，现在则多出了“IMAGES”栏对应图像的可视化。\n",
    "左侧的滑动按钮可以调整图像的亮度和对比度。\n",
    "\n",
    "此外，除了可视化原始图像，TensorBoard提供的可视化方案自然也适用于我们在Python中用matplotlib等工具绘制的其它图像，用于展示分析结果等内容\n",
    "\n",
    "#### 3.6 TensorBoard连续变量可视化\n",
    "TensorBoard可以用来可视化连续变量（或时序变量）的变化过程，通过add_scalar实现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=SummaryWriter('./pytorch_tb')\n",
    "for i in range(500):\n",
    "    x=i\n",
    "    y=x**2\n",
    "    writer.add_scalar('x',x,i) #日志中记录x在第step i 的值\n",
    "    writer.add_scalar('y',y,i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化结果如下：\n",
    "![tb_scalar](./figures/figures_visualization/tb_scalar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想在同一张图中显示多个曲线，则需要分别建立存放子路径（使用SummaryWriter指定路径即可自动创建，\n",
    "但需要在tensorboard运行目录下），同时在add_scalar中修改曲线的标签使其`一致`即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1 = SummaryWriter('./pytorch_tb/x')\n",
    "writer2 = SummaryWriter('./pytorch_tb/y')\n",
    "for i in range(500):\n",
    "    x = i\n",
    "    y = x*2\n",
    "    writer1.add_scalar(\"same\", x, i) #日志中记录x在第step i 的值\n",
    "    writer2.add_scalar(\"same\", y, i) #日志中记录y在第step i 的值\n",
    "writer1.close()\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tb_scalar_twolines](./figures/figures_visualization/tb_twolines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里也可以用一个writer，但for循环中不断创建SummaryWriter不是一个好选项。此时左下角的Runs部分出现了勾选项，\n",
    "我们可以选择我们想要的可视化的曲线。曲线名称对应存放子路径的名称（这里是x和y）\n",
    "\n",
    "这部分功能非常适合损失函数的可视化，可以帮助我们更加直观地了解模型的训练情况，从而确定最佳的checkpoint。\n",
    "左侧的Smoothing滑动按钮可以调整曲线的平滑度，当损失函数震荡比较大时，将Smoothing调大有助于观察loss大整体变化趋势\n",
    "\n",
    "#### 3.7 TensorBoard参数分布可视化\n",
    "\n",
    "当我们需要对参数（或向量）的变化，或者对其分布进行研究时，可以方便地用TensorBoard来进行可视化，通过add_histogram实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 创建正态分布的张量模拟参数矩阵\n",
    "def norm(mean,std):\n",
    "    t=std*torch.randn((100,20))+mean\n",
    "    return t\n",
    "\n",
    "writer=SummaryWriter('./pytorch_tb/')\n",
    "for step,mean in enumerate(range(-10,10,1)):\n",
    "    w=norm(mean,1)\n",
    "    writer.add_histogram(\"w\",w,step)\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tb_hist](./figures/figures_visualization/tb_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 总结\n",
    "\n",
    "对于TensorBoard来说，它的功能是很强大的，可以记录的东西不只限于本节所介绍的范围。\n",
    "\n",
    "主要的实现方案是否见一个SummaryWriter，然后通过add_XXX()函数来实现。\n",
    "\n",
    "其实TensorBoard的逻辑还是很简单的，它的基本逻辑就是文件的读写逻辑，写入想要的可视化的数据，然后TensorBoard自己会读出来\n",
    "\n",
    "本节参考\n",
    "\n",
    "> https://blog.csdn.net/Python_Ai_Road/article/details/107704530"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb6143845b92b855b5e88c82ed3f97cd115bc45f8826ec8313c02558355ab44d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
