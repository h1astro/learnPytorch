{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torchvision里常用的库transform，datasets平时用的多，model主要还是自己去github找的，不少模型没有集成到models库里。torchvideo和torchtext之前没接触过，学习了大概的用法。Lightning这个库有看到过，但感觉还是不适应用这个。\n",
    "> Notes written by h1asro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch的强大并不仅局限于自身的易用性，更在于开源社区围绕Pytorch所产生的一系列工具包（一般是python package）程序，\n",
    "这些优秀的工具包极大地方便了pytorch在特定领域的使用。比如对于计算机视觉，有TorchVision，TorchVideo等用于图片和视频处理；\n",
    "对于自然语言处理，有torchtext；\n",
    "对于图卷机网络，有Pytorch Geometric...这里只是举例，每个领域还有很多优秀的工具包供社区使用。\n",
    "这些工具包共同构成了Pytorch的生态（EcoSystem）。\n",
    "\n",
    "Pytorch生态很大程度助力了Pytorch的推广与成功。在特定领域使用Pytorch生态中的工具包，能够极大地降低入门门槛，方便复线已有的工作。\n",
    "比如我们在讨论模型修改时候就用到了torchvision中预定义的resnet结构，而不需要自己重新编写。\n",
    "同时，Pytorch生态有助于社区力量的加入，共同为社区提供更有价值的内容和程序，这也是开源理念所坚持的价值。\n",
    "\n",
    "在后面的内容中，我们会逐步介绍PyTorch生态在图像、视频、文本等领域中的发展，针对某个领域我们选择其中有代表性的一个工具包进行详细介绍，主要包括工具包的作者或其所在机构、数据预处理工具（这块可能再引入第三方工具包）、数据扩增、常用模型结构的预定义、预训练模型权重、常用损失函数、常用评测指标、封装好的训练&测试模块，以及可视化工具。这些内容也是我们在使用对应工具包时会用到的。读者可以根据自身需要重点学习，对于自己研究所不涉及的工具包，可以只做了解，需要使用时再来学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch之所以会在短短的纪念时间里发展成为主流的DL框架，除去框架本身的优势，还在于Pytorch有着良好的生态圈。\n",
    "我们经常会用到torchvision来调用预训练模型，加载数据集，对图片进行数据增强对操作。\n",
    "\n",
    "* 了解torchvision\n",
    "* 了解torchvision的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 torchvision简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\"\n",
    "\n",
    "正如引言介绍的一样，我们可以知道torchvision包含了在计算机视觉中常常用到的数据集，模型和图像处理的方式，\n",
    "而具体的torchvision则包括了下面这几部分，带*的部分是我们经常会使用到的一些库，所以在下面的部分我们对这些库进行一个简单的介绍：\n",
    "\n",
    "* torchvision.datasets *\n",
    "* torchvision.models *\n",
    "* torchvison.transforms *\n",
    "* torchvision.io \n",
    "* torchvison.ops\n",
    "* torchvison.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 torchvison.datasets\n",
    "\n",
    "torchvision.datasets 主要包含了一些在CV中常见的数据集，在==0.10.0版本==的torchvision下，有以下的数据集\n",
    "\n",
    "| Caltech       | CelebA           | CIFAR             | Cityscapes |\n",
    "| ------------- | ---------------- | ----------------- | ---------- |\n",
    "| **EMNIST**    | **FakeData**     | **Fashion-MNIST** | **Flickr** |\n",
    "| **ImageNet**  | **Kinetics-400** | **KITTI**         | **KMNIST** |\n",
    "| **PhotoTour** | **Places365**    | **QMNIST**        | **SBD**    |\n",
    "| **SEMEION**   | **STL10**        | **SVHN**          | **UCF101** |\n",
    "| **VOC**       | **WIDERFace**    |                   |            |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 torchvision.transforms\n",
    "\n",
    "我们知道在CV中处理的数据集有很大一部分是图片类型的，如果获取的数据集格式或者大小不一样的图片，\n",
    "则需要进行归一化和大小缩放等操作，这些是常用的数据预处理方法。除此之外，当图片数据有限时，\n",
    "我们还需要通过对现有图片数据进行各种变换，如缩小或放大、水平或垂直翻转等，这些是常见的数据增强方法，\n",
    "而torchvision.transforms中就包含了许多这样的操作。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "data_transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),# 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上面提到的几种数据增强操作，在torchvision官方文档里提到了更多的操作，具体使用方法也可以参考本节配套的`transforms.ipynb`\n",
    "更多数据变换的操作，可以查看[这里](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 torchvison.models\n",
    "\n",
    "为了提高训练效率，减少不必要的重复劳动，Pytorch官方也提供了一些预训练好的模型供我们使用，可以查看[这里](https://github.com/pytorch/vision/tree/master/torchvision/models)\n",
    "有哪些预训练模型，下面我们将对如何使用这些模型进行详细介绍。\n",
    "此处以torchvision0.10.0为例，如果希望获取更多的预训练模型，可以使用pretrained-models.pytorch仓库。\n",
    "现有预训练好的模型可以分为以下几类：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Classification**\n",
    "\n",
    "在图像分类里面，PyTorch官方提供了以下模型，并正在不断增多。\n",
    "\n",
    "| AlexNet         | VGG              | ResNet        | SqueezeNet        |\n",
    "| --------------- | ---------------- | ------------- | ----------------- |\n",
    "| **DenseNet**    | **Inception v3** | **GoogLeNet** | **ShuffleNet v2** |\n",
    "| **MobileNetV2** | **MobileNetV3**  | **ResNext**   | **Wide ResNet**   |\n",
    "| **MNASNet**     | **EfficientNet** | **RegNet**    | **持续更新**      |\n",
    "\n",
    "这些模型是在ImageNet-1k进行预训练好的，具体的使用我们会在后面进行介绍。\n",
    "除此之外，我们也可以点击[这里](https://pytorch.org/vision/stable/models.html#)去查看这些模型在ImageNet-1k的准确率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Semantic Segmentation**\n",
    "\n",
    "语义分割的预训练模型是在COCO train2017的子集上进行训练的，提供了20个类别，包括background, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa,train, tvmonitor。\n",
    "\n",
    "| **FCN ResNet50**              | **FCN ResNet101**               | **DeepLabV3 ResNet50** | **DeepLabV3 ResNet101** |\n",
    "| ----------------------------- | ------------------------------- | ---------------------- | ----------------------- |\n",
    "| **LR-ASPP MobileNetV3-Large** | **DeepLabV3 MobileNetV3-Large** | **未完待续**           |                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体我们可以点击[这里](https://pytorch.org/vision/stable/models.html#semantic-segmentation)进行查看预训练的模型的`mean IOU`和` global pixelwise acc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Object Detection，instance Segmentation and Keypoint Detection**\n",
    "\n",
    "物体检测，实例分割和人体关键点检测的模型我们同样是在COCO train2017进行训练的，在下方我们提供了实例分割的类别和人体关键点检测类别：\n",
    "\n",
    "```python\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus','train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A','handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball','kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket','bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl','banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza','donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table','N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone','microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book','clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "COCO_PERSON_KEYPOINT_NAMES =['nose','left_eye','right_eye','left_ear','right_ear','left_shoulder','right_shoulder','left_elbow','right_elbow','left_wrist','right_wrist','left_hip','right_hip','left_knee','right_knee','left_ankle','right_ankle']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Faster R-CNN** | **Mask R-CNN** | **RetinaNet** | **SSDlite** |\n",
    "| ---------------- | -------------- | ------------- | ----------- |\n",
    "| **SSD**          | **未完待续**   |               |             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Object Detection，instance Segmentation and Keypoint Detection**\n",
    "\n",
    "物体检测，实例分割和人体关键点检测的模型我们同样是在COCO train2017进行训练的，在下方我们提供了实例分割的类别和人体关键点检测类别：\n",
    "\n",
    "```python\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus','train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A','handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball','kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket','bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl','banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza','donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table','N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone','microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book','clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "COCO_PERSON_KEYPOINT_NAMES =['nose','left_eye','right_eye','left_ear','right_ear','left_shoulder','right_shoulder','left_elbow','right_elbow','left_wrist','right_wrist','left_hip','right_hip','left_knee','right_knee','left_ankle','right_ankle']\n",
    "```\n",
    "\n",
    "| **Faster R-CNN** | **Mask R-CNN** | **RetinaNet** | **SSDlite** |\n",
    "| ---------------- | -------------- | ------------- | ----------- |\n",
    "| **SSD**          | **未完待续**   |               |             |\n",
    "\n",
    "同样的，我们可以点击[这里](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)查看这些模型在COCO train 2017上的`box AP`,`keypoint AP`,`mask AP`\n",
    "\n",
    "- **Video classification**\n",
    "\n",
    "视频分类模型是在 Kinetics-400上进行预训练的\n",
    "\n",
    "| **ResNet 3D 18** | **ResNet MC 18** | **ResNet (2+1) D** |\n",
    "| ---------------- | ---------------- | ------------------ |\n",
    "| **未完待续**     |                  |                    |\n",
    "\n",
    "同样我们也可以点击[这里](https://pytorch.org/vision/stable/models.html#video-classification)查看这些模型的`Clip acc@1`,`Clip acc@5`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 torchvision.io\n",
    "\n",
    "在`torchvision.io`提供了视频、图片和文件的IO操作的功能，它们包括读取、写入、编解码处理操作。\n",
    "随着torchvision的发展，io也增加了更多底层的高效率的API。\n",
    "在使用torchvision.io的过程中，我们需要注意以下几点：\n",
    "\n",
    "- **不同版本之间**，`torchvision.io`有着较大变化，因此在使用时，需要查看下我们的`torchvision`版本是否存在你想使用的方法。\n",
    "- 除了read_video()等方法，torchvision.io为我们提供了一个细粒度的视频API torchvision.io.VideoReader()  ，它具有更高的效率并且更加接近底层处理。在使用时，我们需要先安装ffmpeg然后从源码重新编译torchvision我们才能我们能使用这些方法。\n",
    "- 在使用Video相关API时，我们最好提前安装好PyAV这个库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 torchvision.ops\n",
    "\n",
    "torchvision.ops为我们提供了许多CV的特定操作，包括但不仅限于NMS，RoIAlign（Mask R-CNN中应用的一种方法），\n",
    "RoIPool（Fast R-CNN中用到的一种方法）。在合适的时间使用可以大大降低工作量，避免重复的造轮子，[这里](https://pytorch.org/vision/stable/ops.html)查看更多函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 torchvision.utils\n",
    "\n",
    "提供了一些可视化的方法，帮助我们将若干张图片拼接在一起、可视化检测和分割的效果。\n",
    "具体函数查看[这里](https://pytorch.org/vision/stable/utils.html)\n",
    "\n",
    "总的来说，torchvision的出现帮助我们解决了常见的CV中一些重复耗时的工作，\n",
    "并在数据集的获取、数据增强、模型预训练等方面大大降低了工作难度，可以让我们更加快速上手一些CV任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PytorchVideo简介\n",
    "\n",
    "![](./figures/ecology/logo2.jpg)\n",
    "\n",
    "近几年来，随着传播媒介和视频平台的发展，视频正在取代图片成为下一代的主流媒体，\n",
    "这也使得有关视频的DL模型正在获得越来越多的关注。\n",
    "然而，有关视频的DL模型仍然有着许多缺点：\n",
    "\n",
    "- 计算资源耗费更多，并且没有高质量的`model zoo`，不能像图片一样进行迁移学习和论文复现。\n",
    "- 数据集处理较麻烦，但没有一个很好的视频处理工具\n",
    "- 随着多模态越来越流行，亟需一个工具来处理其它模态\n",
    "\n",
    "除此之外，还有部署优化等问题，为了解决这些问题，Meta推出了`PytorchVideo`DL库（包含组件如Figure 1所示）。\n",
    "PytorchVideo是一个专注于视频理解工作等DL库。\n",
    "PytorchVideo提供了加速视频理解研究所需的可用性、模块化和高效的组件。\n",
    "PytorchVideo是使用[PyTorch](https://pytorch.org/)开发的，\n",
    "支持不同的深度学习视频组件，如视频模型、视频数据集和视频特定转换。\n",
    "\n",
    "![](./figures/ecology/list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 PytorchVideo的主要部件和亮点\n",
    "\n",
    "PytorchVideo提供了加速视频理解研究所需的模块化和高效的API。\n",
    "它还支持不同的DL视频组件，如视频模型、视频数据和视频特定转换，最重要的是，\n",
    "PytorchVideo也提供了model zoo，使得人们可以使用各种先进的预训练视频模型及其评判基准。\n",
    "PytorchVideo主要亮点如下：\n",
    "\n",
    "- **基于 PyTorch：**使用 PyTorch 构建。使所有 PyTorch 生态系统组件的使用变得容易。\n",
    "- **Model Zoo：**PyTorchVideo提供了包含I3D、R(2+1)D、SlowFast、X3D、MViT等SOTA模型的高质量model zoo（\n",
    "目前还在快速扩充中，未来会有更多SOTA model），\n",
    "并且PyTorchVideo的model zoo调用与\n",
    "[PyTorch Hub](pytorch.org/hub/)做了整合，\n",
    "大大简化模型调用，具体的一些调用方法可以参考下面的【使用 PyTorchVideo model zoo】部分。\n",
    "\n",
    "- **数据预处理和常见数据**，PyTorchVideo支持Kinetics-400, Something-Something V2, Charades, Ava (v2.2), \n",
    "Epic Kitchen, HMDB51, UCF101, Domsev等主流数据集和相应的数据预处理，\n",
    "同时还支持randaug, augmix等数据增强trick。\n",
    "\n",
    "- **模块化设计**：PyTorchVideo的设计类似于torchvision，\n",
    "也是提供许多模块方便用户调用修改，\n",
    "在PyTorchVideo中具体来说包括data, transforms, layer, model, accelerator等模块，\n",
    "方便用户进行调用和读取。\n",
    "\n",
    "- **支持多模态**：PyTorchVideo现在对多模态的支持包括了visual和audio，未来会支持更多模态，为多模态模型的发展提供支持。\n",
    "\n",
    "- **移动端部署优化**：PyTorchVideo支持针对移动端模型的部署优化（使用前述的PyTorchVideo/accelerator模块），\n",
    "模型经过PyTorchVideo优化了最高达**7倍**的提速，并实现了第一个能实时跑在手机端的X3D模型（实验中可以实时跑在2018年的三星Galaxy S8上，\n",
    "具体请见[Android Demo APP](https://github.com/pytorch/android-demo-app/tree/master/TorchVideo)）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 PytorchVideo的安装\n",
    "我们可以直接使用pip来安装PyTorchVideo\n",
    "\n",
    "```shell\n",
    "pip install pytorchvideo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：\n",
    "\n",
    "- 安装的虚拟环境的python版本 >= 3.7\n",
    "- PyTorch >= 1.8.0，安装的torchvision也需要匹配\n",
    "- CUDA >= 10.2\n",
    "- ioPath：[具体情况](https://github.com/facebookresearch/iopath)\n",
    "- fvcore版本 >= 0.1.4：[具体情况](https://github.com/facebookresearch/fvcore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Model zoo 和 benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面这部分，我将简单介绍些PyTorchVideo所提供的Model zoo和benchmark\n",
    "\n",
    "- Kinetics-400\n",
    "\n",
    "| arch     | depth | pretrain | frame length x sample rate | top 1 | top 5 | Flops (G) x views | Params (M) | Model                                                        |\n",
    "| -------- | ----- | -------- | -------------------------- | ----- | ----- | ----------------- | ---------- | ------------------------------------------------------------ |\n",
    "| C2D      | R50   | \\-       | 8x8                        | 71.46 | 89.68 | 25.89 x 3 x 10    | 24.33      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/C2D\\_8x8\\_R50.pyth) |\n",
    "| I3D      | R50   | \\-       | 8x8                        | 73.27 | 90.70 | 37.53 x 3 x 10    | 28.04      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/I3D\\_8x8\\_R50.pyth) |\n",
    "| Slow     | R50   | \\-       | 4x16                       | 72.40 | 90.18 | 27.55 x 3 x 10    | 32.45      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW\\_4x16\\_R50.pyth) |\n",
    "| Slow     | R50   | \\-       | 8x8                        | 74.58 | 91.63 | 54.52 x 3 x 10    | 32.45      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW\\_8x8\\_R50.pyth) |\n",
    "| SlowFast | R50   | \\-       | 4x16                       | 75.34 | 91.89 | 36.69 x 3 x 10    | 34.48      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST\\_4x16\\_R50.pyth) |\n",
    "| SlowFast | R50   | \\-       | 8x8                        | 76.94 | 92.69 | 65.71 x 3 x 10    | 34.57      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST\\_8x8\\_R50.pyth) |\n",
    "| SlowFast | R101  | \\-       | 8x8                        | 77.90 | 93.27 | 127.20 x 3 x 10   | 62.83      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST\\_8x8\\_R101.pyth) |\n",
    "| SlowFast | R101  | \\-       | 16x8                       | 78.70 | 93.61 | 215.61 x 3 x 10   | 53.77      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST\\_16x8\\_R101_50_50.pyth) |\n",
    "| CSN      | R101  | \\-       | 32x2                       | 77.00 | 92.90 | 75.62 x 3 x 10    | 22.21      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/CSN\\_32x2\\_R101.pyth) |\n",
    "| R(2+1)D  | R50   | \\-       | 16x4                       | 76.01 | 92.23 | 76.45 x 3 x 10    | 28.11      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/R2PLUS1D\\_16x4\\_R50.pyth) |\n",
    "| X3D      | XS    | \\-       | 4x12                       | 69.12 | 88.63 | 0.91 x 3 x 10     | 3.79       | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D\\_XS.pyth) |\n",
    "| X3D      | S     | \\-       | 13x6                       | 73.33 | 91.27 | 2.96 x 3 x 10     | 3.79       | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D\\_S.pyth) |\n",
    "| X3D      | M     | \\-       | 16x5                       | 75.94 | 92.72 | 6.72 x 3 x 10     | 3.79       | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D\\_M.pyth) |\n",
    "| X3D      | L     | \\-       | 16x5                       | 77.44 | 93.31 | 26.64 x 3 x 10    | 6.15       | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D\\_L.pyth) |\n",
    "| MViT     | B     | \\-       | 16x4                       | 78.85 | 93.85 | 70.80 x 1 x 5     | 36.61      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/MVIT\\_B\\_16x4.pyth) |\n",
    "| MViT     | B     | \\-       | 32x3                       | 80.30 | 94.69 | 170.37 x 1 x 5    | 36.61      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/MVIT\\_B\\_32x3\\_f294077834.pyth) |\n",
    "\n",
    "- Something-Something V2\n",
    "\n",
    "| arch     | depth | pretrain     | frame length x sample rate | top 1 | top 5 | Flops (G) x views | Params (M) | Model                                                        |\n",
    "| -------- | ----- | ------------ | -------------------------- | ----- | ----- | ----------------- | ---------- | ------------------------------------------------------------ |\n",
    "| Slow     | R50   | Kinetics 400 | 8x8                        | 60.04 | 85.19 | 55.10 x 3 x 1     | 31.96      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ssv2/SLOW\\_8x8\\_R50.pyth) |\n",
    "| SlowFast | R50   | Kinetics 400 | 8x8                        | 61.68 | 86.92 | 66.60 x 3 x 1     | 34.04      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ssv2/SLOWFAST\\_8x8\\_R50.pyth) |\n",
    "\n",
    "- Charades\n",
    "\n",
    "| arch     | depth | pretrain     | frame length x sample rate | MAP   | Flops (G) x views | Params (M) | Model                                                        |\n",
    "| -------- | ----- | ------------ | -------------------------- | ----- | ----------------- | ---------- | ------------------------------------------------------------ |\n",
    "| Slow     | R50   | Kinetics 400 | 8x8                        | 34.72 | 55.10 x 3 x 10    | 31.96      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/charades/SLOW\\_8x8\\_R50.pyth) |\n",
    "| SlowFast | R50   | Kinetics 400 | 8x8                        | 37.24 | 66.60 x 3 x 10    | 34.00      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/charades/SLOWFAST\\_8x8\\_R50.pyth) |\n",
    "\n",
    "- AVA (V2.2)\n",
    "\n",
    "| arch     | depth | pretrain     | frame length x sample rate | MAP   | Params (M) | Model                                                        |\n",
    "| -------- | ----- | ------------ | -------------------------- | ----- | ---------- | ------------------------------------------------------------ |\n",
    "| Slow     | R50   | Kinetics 400 | 4x16                       | 19.5  | 31.78      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ava/SLOW\\_4x16\\_R50\\_DETECTION.pyth) |\n",
    "| SlowFast | R50   | Kinetics 400 | 8x8                        | 24.67 | 33.82      | [link](https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ava/SLOWFAST\\_8x8\\_R50\\_DETECTION.pyth) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 使用 PyTorchVideo model zoo\n",
    "\n",
    "PytorchVideo提供了三种使用方法，并且给每一种都配备了`tutorial`\n",
    "\n",
    "* TorchHub，这些模型都已经在TorchHub存在。\n",
    "我们可以根据实际情况来选择需不需要使用预训练模型。除此之外，官方也给出了TorchHub使用的 [tutorial](https://pytorchvideo.org/docs/tutorial_torchhub_inference) 。\n",
    "\n",
    "* PySlowFast，使用 [PySlowFast workflow](https://github.com/facebookresearch/SlowFast/) 去训练或测试PyTorchVideo models/datasets.\n",
    "\n",
    "* [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)建立一个工作流进行处理，点击查看官方 [tutorial](https://pytorchvideo.org/docs/tutorial_classification)。\n",
    "\n",
    "- 如果想查看更多的使用教程，可以点击 [这里](https://github.com/facebookresearch/pytorchvideo/tree/main/tutorials) 进行尝试\n",
    "\n",
    "总的来说，PyTorchVideo的使用与torchvision的使用方法类似，在有了前面的学习基础上，我们可以很快上手PyTorchVideo，具体的我们可以通过查看官方提供的文档和一些例程来了解使用方法：[官方网址](https://pytorchvideo.readthedocs.io/en/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 torchtext简介\n",
    "\n",
    "本节介绍PyTorch官方用语自然语言处理的工具包torchtext。NLP也是DL的一大应用场景，\n",
    "近年来随着大规模预训练模型的应用，深度学习和人机对话、及其翻译等领域取得了非常好的效果，\n",
    "也使得NLP相关的DL模型获得了越来越多的关注。\n",
    "\n",
    "由于NLP和CV在数据预处理中的不同，因此NLP的工具包torchtext和torchvision等CV相关的工具包也有一些功能上的差异，如：\n",
    "\n",
    "- 数据集（dataset）定义方式不同\n",
    "- 数据预处理工具\n",
    "- 没有琳琅满目的model zoo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节介绍参考了[atnlp的Github](https://github.com/atnlp/torchtext-summary)，在此致谢！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 torchtext的主要组成部分\n",
    "\n",
    "torchtext可以方便的对文本进行预处理，例如截断补长、构建词表等。\n",
    "torchtext主要包含了以下的主要组成部分：\n",
    "\n",
    "- 数据处理工具 torchtext.data.functional、torchtext.data.utils\n",
    "- 数据集 torchtext.data.datasets\n",
    "- 词表工具 torchtext.vocab\n",
    "- 评测指标 torchtext.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 torchtext的安装\n",
    "\n",
    "torchtext可以直接使用pip进行安装：\n",
    "\n",
    "```bash\n",
    "pip install torchtext\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 构建数据库\n",
    "\n",
    "- **Field及其使用**\n",
    "\n",
    "Field是torchtext中定义数据类型以及转换为张量的指令。\n",
    "`torchtext`认为一个样本是由多个字段（文本字段，标签字段）组成，不同的字段可能会有不同的处理方式，\n",
    "所以才会有`Field`抽象。定义Field对象是为了明确如何处理不同类型的数据，但具体的处理则是在Dataset中完成的。\n",
    "下面通过一个例子来简要说明Field的使用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize=lambda x: x.split()\n",
    "TEXT=data.Field(sequential=True,tokenize=tokenize,lower=True,fix_length=200)\n",
    "LABEL=data.Field(sequential=False,use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "​\tsequential设置数据是否是顺序表示的；\n",
    "\n",
    "​\ttokenize用于设置将字符串标记为顺序实例的函数\n",
    "\n",
    "​\tlower设置是否将字符串全部转为小写；\n",
    "\n",
    "​\tfix_length设置此字段所有实例都将填充到一个固定的长度，方便后续处理；\n",
    "\n",
    "​\tuse_vocab设置是否引入Vocab object，如果为False，则需要保证之后输入field中的data都是numerical的\n",
    "\n",
    "构建Field完成后就可以进一步构建dataset了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "def get_dataset(csv_data,text_field,label_field,test=False):\n",
    "    fields=[('id',None),\n",
    "            ('comment_text',text_field),('toxic',label_field)\n",
    "    ]\n",
    "    examples=[]\n",
    "    if test:\n",
    "        # 如果为测试集，则不加载label\n",
    "        for text in tqdm(csv_data['comment_text']):\n",
    "            examples.append(data.Example.fromlist([None,text,None],fields))\n",
    "    else:\n",
    "        for text,label in tqdm(zip(csv_data['comment_text'],csv_data['toxic'])):\n",
    "            examples.append(data.Example.fromlist([None,text,label],fields))\n",
    "    \n",
    "    return examples,fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用数据csv_data中有“comment_text”和“toxic”两列，分别对应text和label，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train_toxic_comments.csv')\n",
    "valid_data=pd.read_csv('valid_toxic_comments.csv')\n",
    "test_data=pd.rad_csv('test_toxic_comments.csv')\n",
    "TEXT=data.Field(sequential=True,tokenize=tokenize,lower=True)\n",
    "LABEL=data.Field(sequential=False,use_vocab=False)\n",
    "\n",
    "# 得到构建Dataset所需的examples和fields\n",
    "train_examples,train_fields=get_dataset(train_data,TEXT,LABEL)\n",
    "valid_examples,valid_fields=get_dataset(valid_data,TEXT,LABEL)\n",
    "test_examples,test_fields=get_dataset(test_data,TEXT,None,test=True)\n",
    "# 构建Dataset数据集\n",
    "train=data.DataSet(train_examples,train_fields)\n",
    "valid=data.Dataset(valid_examples,valid_fields)\n",
    "test=data.Dataset(test_examples,test_fields)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，定义Field对象完成后，通过get_dataset函数可以读如数据的文本和标签，将二者（examples）\n",
    "联通field一起送到torchtext.data.Dataset类中，即可完成数据集的构建。\n",
    "使用以下命令可以看下读入的数据情况：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查keys是否正确\n",
    "print(train[0].__dict__.keys())\n",
    "print(test[0].__dict__.keys())\n",
    "# 抽查内容是否正确\n",
    "print(train[0].comment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **词汇表（vocab）**\n",
    "\n",
    "在NLP中，将字符串形式的词语（word）转变为数字形式的向量表示（embedding）是非常重要的一步，被称为Word Embedding。这一步的基本思想是收集一个比较大的语料库（尽量与所做的任务相关），在语料库中使用word2vec之类的方法构建词语到向量（或数字）的映射关系，之后将这一映射关系应用于当前的任务，将句子中的词语转为向量表示。\n",
    "\n",
    "在torchtext中可以使用Field自带的build_vocab函数完成词汇表构建。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **数据迭代器**\n",
    "\n",
    "其实就是torchtext中的DataLoader，看下代码就明白了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "# 若只针对训练集构造迭代器\n",
    "# train_iter = data.BucketIterator(dataset=train, batch_size=8, shuffle=True, sort_within_batch=False, repeat=False)\n",
    "\n",
    "# 同时对训练集和验证集进行迭代器的构建\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (train, valid), # 构建数据集所需的数据集\n",
    "        batch_sizes=(8, 8),\n",
    "        device=-1, # 如果使用gpu，此处将-1更换为GPU的编号\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False\n",
    ")\n",
    "\n",
    "test_iter = Iterator(test, batch_size=8, device=-1, sort=False, sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext支持只对一个dataset和同时对多个dataset构建数据迭代器。\n",
    "\n",
    "- **使用自带数据集**\n",
    "\n",
    "与torchvision类似，torchtext也提供若干常用的数据集方便快速进行算法测试。可以查看[官方文档](https://pytorch.org/text/stable/datasets.html)寻找想要使用的数据集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 评测指标（metric）\n",
    "\n",
    "NLP中部分任务的评测不是通过准确率等指标完成的，比如机器翻译任务常用BLEU (bilingual evaluation understudy) score来评价预测文本和标签文本之间的相似程度。torchtext中可以直接调用torchtext.data.metrics.bleu_score来快速实现BLEU，下面是一个官方文档中的一个例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
    "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
    "bleu_score(candidate_corpus, references_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 其他\n",
    "\n",
    "值得注意的是，由于NLP常用的网络结构比较固定，torchtext并不像torchvision那样提供一系列常用的网络结构。模型主要通过torch.nn中的模块来实现，比如torch.nn.LSTM、torch.nn.RNN等。\n",
    "\n",
    "**备注：**\n",
    "\n",
    "对于文本研究而言，当下Transformer已经成为了绝对的主流，因此PyTorch生态中的[HuggingFace](https://huggingface.co/)等工具包也受到了越来越广泛的关注。这里强烈建议读者自行探索相关内容，可以写下自己对于HuggingFace的笔记，如果总结全面的话欢迎pull request，充实我们的课程内容。\n",
    "\n",
    "**本节参考**\n",
    "\n",
    "- [torchtext官方文档](https://pytorch.org/text/stable/index.html)\n",
    "- [atnlp/torchtext-summary](https://github.com/atnlp/torchtext-summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
